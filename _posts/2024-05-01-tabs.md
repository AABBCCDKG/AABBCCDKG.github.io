---
layout: post
title: Top Research Labs in Machine Learning Systems (MLSys)
date: 2024-05-01 00:32:13
description: A brief introduction to top-tier MLSys labs for PhD applications.
tags: formatting code
categories: sample-posts
tabs: true
---

## Google Brain and Microsoft Research (MSR)
Location: Various locations in the USA
Focus Areas: Distributed systems, deep learning optimization, hardware-software co-design
Highlights: Google Brain and MSR are giants in both academia and industry, contributing significantly to advancements in machine learning infrastructure. Their work spans optimizing deep learning models, developing new frameworks like TensorFlow and PyTorch, and exploring the intersection of ML and systems.
## Catalyst Group at Carnegie Mellon University (CMU)
Location: Pittsburgh, PA
Key Researchers: Eric Xing, Tianqi Chen, Zhihao Jia
Focus Areas: High-performance machine learning, systems for large-scale ML
Highlights: Catalyst Group at CMU is known for pioneering projects like TVM, XGBoost, and TASO, focusing on optimizing machine learning pipelines for performance and scalability. Their work has significant implications for both academic research and industrial applications.
## RISELab at University of California, Berkeley
Location: Berkeley, CA
Key Researchers: Ion Stoica, Michael Jordan
Focus Areas: Distributed computing, AI for systems, systems for AI
Highlights: RISELab is famous for its groundbreaking work on Ray, an open-source project for building distributed applications, and for Spark, which revolutionized big data processing. The lab continues to explore how AI can be leveraged to improve system performance and reliability.
## Stanford DAWN Project
Location: Stanford, CA
Key Researchers: Matei Zaharia, Peter Bailis
Focus Areas: Systems for deep learning, AI infrastructure
Highlights: Stanford's DAWN project brings together experts in machine learning and systems to create scalable, efficient AI solutions. The group has developed innovative techniques for optimizing deep learning workflows, making AI more accessible and cost-effective.
## System Lab at University of Washington (UW)
Location: Seattle, WA
Key Researchers: Luis Ceze, Arvind Krishnamurthy
Focus Areas: AI for systems, systems for AI, serverless computing
Highlights: The UW System Lab focuses on the intersection of AI and systems, with notable contributions in serverless computing and network optimizations for distributed machine learning. Their work has practical implications for cloud computing and AI infrastructure.
## SymbioticLab at University of Michigan
Location: Ann Arbor, MI
Key Researchers: Mosharaf Chowdhury
Focus Areas: Distributed systems, networking for AI, big data systems
Highlights: SymbioticLab is dedicated to exploring how distributed systems can be optimized to support large-scale AI workloads. Their research spans from efficient data communication protocols to innovative scheduling algorithms, enhancing the performance of big data and machine learning systems.
## MIT CSAIL (Computer Science and Artificial Intelligence Laboratory)
Location: Cambridge, MA
Key Researchers: Tim Kraska, Saman Amarasinghe
Focus Areas: Data systems, learned index structures, sparse computing
Highlights: MIT CSAILâ€™s contributions to MLSys include learned index structures that optimize database queries and innovative methods for sparse computing. Their research is fundamental in pushing the boundaries of what is possible in machine learning systems.
## EcoSystem at University of Toronto
Location: Toronto, Canada
Key Researchers: Gennady Pekhimenko
Focus Areas: Machine learning system optimization, hardware acceleration for ML
Highlights: The EcoSystem lab at UToronto focuses on optimizing machine learning systems through hardware acceleration and efficient software design. Their research helps bridge the gap between cutting-edge ML algorithms and practical, deployable systems.


This is how a post with [tabs](https://github.com/Ovski4/jekyll-tabs) looks like. Note that the tabs could be used for different purposes, not only for code.

## First tabs

To add tabs, use the following syntax:

{% raw %}

```liquid
{% tabs group-name %}

{% tab group-name tab-name-1 %}

Content 1

{% endtab %}

{% tab group-name tab-name-2 %}

Content 2

{% endtab %}

{% endtabs %}
```

{% endraw %}

With this you can generate visualizations like:

{% tabs log %}

{% tab log php %}

```php
var_dump('hello');
```

{% endtab %}

{% tab log js %}

```javascript
console.log("hello");
```

{% endtab %}

{% tab log ruby %}

```javascript
pputs 'hello'
```

{% endtab %}

{% endtabs %}

## Another example

{% tabs data-struct %}

{% tab data-struct yaml %}

```yaml
hello:
  - "whatsup"
  - "hi"
```

{% endtab %}

{% tab data-struct json %}

```json
{
  "hello": ["whatsup", "hi"]
}
```

{% endtab %}

{% endtabs %}

## Tabs for something else

{% tabs something-else %}

{% tab something-else text %}

Regular text

{% endtab %}

{% tab something-else quote %}

> A quote

{% endtab %}

{% tab something-else list %}

Hipster list

- brunch
- fixie
- raybans
- messenger bag

{% endtab %}

{% endtabs %}
