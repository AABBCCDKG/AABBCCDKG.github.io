<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://aabbccdkg.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://aabbccdkg.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-02T02:57:01+00:00</updated><id>https://aabbccdkg.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">博士申请记录(1)</title><link href="https://aabbccdkg.github.io/blog/2024/phd-application/" rel="alternate" type="text/html" title="博士申请记录(1)"/><published>2024-09-01T00:32:13+00:00</published><updated>2024-09-01T00:32:13+00:00</updated><id>https://aabbccdkg.github.io/blog/2024/phd-application</id><content type="html" xml:base="https://aabbccdkg.github.io/blog/2024/phd-application/"><![CDATA[<p>今天是2024年9月1日。我计划申请2026年开始的博士学位。按照往年情况推算，申请材料会在2025年12月开始陆续被要求提交，面试将在2026年1月底前结束。因此，大概有15个月的时间来准备申请。我希望通过这个博客系列，记录下申请过程。</p> <div style="text-align: center;"> （序章） </div> <p>博士申请中最重要的两个因素是人脉(Connection)和契合度(Match)。</p> <p>申请者很容易夸大自己的简历，因此对博士生导师来说，来自值得信任之人的推荐信至关重要，它意味着推荐人用自己的学术声誉担保该申请者经历的真实性。</p> <p>大多数博士生导师都面临着发表论文的压力，因此他们通常更倾向于那些了解自己研究方向并能快速进入工作状态的候选人，因为大部分科研工作是努力大于天赋。</p> <p>所以我认为，比较有效的申博方式，是在心仪的课题组进行Research Intern，拿Return Offer。一来推荐人就是自己的博士生导师，二来可以保证科研方向100%和导师契合。</p> <p>我读Ph.D.的初心并非出于对科研的热爱，而是希望拥有名校的Doctor头衔。实际上，目前最渴望的事是创业。因此，对于Ph.D.的申请来说，我希望用最有效的方式 - 根据心仪的导师，定制自己的背景。</p> <p>目前，我心仪的导师是Stanford的Christos和Kunle，以及MIT的Daniel。我的计划是在Prof. Muhammad课题组中参与发表一篇论文，并通过他的帮助，争取在今年寒假（12月）之前加入Christos和Kunle的研究组，担任Research Intern，同时密切关注Daniel实验室的进展。在接下来长达11个月的Research Intern中，努力争取获得Return Offer。因此，接下来的博客将分为两部分：Prof. Muhammad课题组的科研进展；心仪导师实验室的最新动态。</p> <div style="text-align: center;"> （科研进展） </div> <p>我有可能加入Ultima这个项目(Accelerating the training process for cloud-based distributed machine learning systems), 但是已经在Archive上了，我的名字还有机会添加上去么？ Ultima似乎和Chritofos的方向比较契合？都是cloud computing</p> <div style="text-align: center;"> （动态） </div> <p>下周和Kunle的学生Nathan Zhang进行Zoom，我应该问些什么？</p>]]></content><author><name></name></author><category term="phd-application"/><category term="phd-application"/><summary type="html"><![CDATA[今天是2024年9月1日。我计划申请2026年开始的博士学位。按照往年情况推算，申请材料会在2025年12月开始陆续被要求提交，面试将在2026年1月底前结束。因此，大概有15个月的时间来准备申请。我希望通过这个博客系列，记录下申请过程。]]></summary></entry><entry><title type="html">Top Research Labs in Machine Learning Systems (MLSys) 🔬</title><link href="https://aabbccdkg.github.io/blog/2024/mlsys-labs-intro/" rel="alternate" type="text/html" title="Top Research Labs in Machine Learning Systems (MLSys) 🔬"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://aabbccdkg.github.io/blog/2024/mlsys-labs-intro</id><content type="html" xml:base="https://aabbccdkg.github.io/blog/2024/mlsys-labs-intro/"><![CDATA[<p>This is a brief introduction to top-tier MLSys labs for Ph.D. applicants, with a primary focus on those in the United States and mainland China. If there are any omissions, please feel free to <a href="mailto:wangdong0502@gmail.com">contact me</a> to add them.</p> <h1 id="-united-states">🇺🇸 United States</h1> <h2 id="google-brain"><a href="https://research.google/">Google Brain</a></h2> <h2 id="microsoft-research-msr"><a href="https://www.microsoft.com/en-us/research/">Microsoft Research (MSR)</a></h2> <ul> <li><a href="https://www.microsoft.com/en-us/research/project/fiddle/">Fast and Efficient Infrastructure for Distributed Deep Learning (Fiddle)</a></li> </ul> <h2 id="catalyst-carnegie-mellon-university"><a href="https://catalyst.cs.cmu.edu/">Catalyst (Carnegie Mellon University)</a></h2> <ul> <li> <p><a href="https://www.cs.cmu.edu/~epxing/">Eric Xing: </a> Prof. Eric was a student of <a href="https://people.eecs.berkeley.edu/~jordan/">Prof. Michael I. Jordan</a>.</p> </li> <li> <p><a href="https://tqchen.com/">Tianqi Chen: </a></p> <ul> <li> <p><a href="https://tvm.apache.org/">TVM: </a> TVM is an open-source framework for optimizing and deploying deep learning models, with its name derived from “Tensor Virtual Machine.” Its primary goal is to optimize and compile deep learning models in an automated manner, enabling efficient execution on various hardware platforms such as CPUs, GPUs, FPGAs, and specialized AI accelerators.</p> </li> <li> <p><a href="https://xgboost.readthedocs.io/en/stable/">XGBoost: </a>XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.</p> </li> <li> <p><a href="https://github.com/apache/mxnet">MXNet: </a>Apache MXNet is a deep learning framework designed for both efficiency and flexibility. It allows you to mix symbolic and imperative programming to maximize efficiency and productivity. At its core, MXNet contains a dynamic dependency scheduler that automatically parallelizes both symbolic and imperative operations on the fly. A graph optimization layer on top of that makes symbolic execution fast and memory efficient. MXNet is portable and lightweight, scalable to many GPUs and machines.</p> </li> </ul> </li> <li> <p><a href="https://www.cs.cmu.edu/~zhihaoj2/">Zhihao Jia: </a> Prof. Zhihao Jia is a student of <a href="https://people.eecs.berkeley.edu/~matei/">Prof. Matei Zaharia (now at UC Berkeley)</a>. He seems to be more focused on system for LLM.</p> <ul> <li><a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://cs.stanford.edu/~padon/taso-sosp19.pdf">TASO: </a></li> <li><a href="https://flexflow.ai/">Flexflow: </a></li> </ul> </li> </ul> <h2 id="dsail-mit"><a href="https://dsail.csail.mit.edu/">DSAIL (MIT)</a></h2> <ul> <li> <p><a href="https://hanlab.mit.edu/songhan">Song Han: </a>Pruning and Sparse related work. Prof. Song Han seems to be working on algorithm modifications and hardware, and he’s not really focused on TinyML anymore. Now he’s working on diffusion models and LLM models.</p> </li> <li> <p><a href="https://people.csail.mit.edu/kraska/">Tim Kraska: </a></p> </li> </ul> <h2 id="csail-mit"><a href="https://www.csail.mit.edu/">CSAIL (MIT)</a></h2> <ul> <li> <p><a href="https://people.csail.mit.edu/kraska/">Tim Kraska: </a> Learned Index</p> </li> <li> <p><a href="https://www.csail.mit.edu/person/saman-amarasinghe">Saman Amarasinghe: </a></p> <ul> <li><a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://people.csail.mit.edu/jrk/halide-pldi13.pdf">Halide</a></li> <li><a href="https://tacos.libraries.mit.edu/">TACO</a></li> </ul> </li> </ul> <h2 id="dawn-project-stanford"><a href="https://dawn.cs.stanford.edu/">DAWN Project (Stanford)</a></h2> <ul> <li><a href="https://people.eecs.berkeley.edu/~matei/">Matei Zaharia (now at UC Berkeley): </a> Prof. Matei Zaharia (Stanford &amp; Databricks) is highly respected for building <a href="https://spark.apache.org/">Apache Spark</a> (one of the most widely used frameworks for distributed data processing, and co-started other datacenter software such as Apache Mesos and Spark Streaming) from scratch to a billion-dollar level. He serves as a PC and chair for major conferences. PipeDream, TASO, and FlexFlow is the project built by his Ph.D. student Zhihao Jia. One standout aspect of his research is that it addresses real system needs, making it impactful and practical. Not all his work prioritizes performance; for instance, one recent paper discusses offloading computation to GPUs using annotation for ease of use. Overall, pursuing a PhD under his guidance would likely lead to significant influence in the industry.</li> </ul> <h2 id="hazy-research-stanford-ai-lab"><a href="https://hazyresearch.stanford.edu/index">Hazy Research (Stanford AI Lab)</a></h2> <p>This research group focuses on MLSys and also organized a seminar series called <a href="https://mlsys.stanford.edu/">Stanford MLSys Seminar Series</a>.</p> <h2 id="riselab-university-of-california-berkeley"><a href="https://rise.cs.berkeley.edu/">RISELab (University of California, Berkeley)</a></h2> <ul> <li> <p><a href="https://people.eecs.berkeley.edu/~istoica/">Ion Stoica: </a></p> </li> <li> <p><a href="https://people.eecs.berkeley.edu/~jordan/">Michael Jordan: </a></p> </li> </ul> <p>Most recent project: <a href="https://rise.cs.berkeley.edu/projects/ray/">Ray</a></p> <p>Professors at RISE Lab have offered a course called <a href="https://ucbrise.github.io/cs294-ai-sys-fa19/">AI for Systems and Systems for AI (CS294)</a>.</p> <h2 id="system-lab-university-of-washington"><a href="https://www.cs.washington.edu/research/systems">System Lab (University of Washington)</a></h2> <ul> <li><a href="https://homes.cs.washington.edu/~luisceze/">Luis Ceze: </a> Prof. Luis Ceze focuses on Programming Language and Computer Architecture. <ul> <li><a href="https://tvm.apache.org/">TVM: </a> TVM is an open-source framework for optimizing and deploying deep learning models, with its name derived from “Tensor Virtual Machine.” Its primary goal is to optimize and compile deep learning models in an automated manner, enabling efficient execution on various hardware platforms such as CPUs, GPUs, FPGAs, and specialized AI accelerators.</li> </ul> </li> <li><a href="https://www.cs.washington.edu/people/faculty/arvind">Arvind Krishnamurthy: </a> Prof. Arvind Krishnamurthy primarily focuses on computer networks. His work involves applying networking technology to address challenges in distributed machine learning. So there is always cutting-edge support in the field of networking.</li> </ul> <h2 id="sample-university-of-washington"><a href="https://sampl.cs.washington.edu/">Sample (University of Washington)</a></h2> <h2 id="symbioticlab-university-of-michigan-ann-arbor"><a href="https://symbioticlab.org/">SymbioticLab (University of Michigan, Ann Arbor)</a></h2> <ul> <li><a href="https://www.mosharaf.com/">Mosharaf Chowdhury (the academic leader): </a> Prof. Mosharaf is a student of <a href="https://people.eecs.berkeley.edu/~istoica/">Prof. Ion Stoica</a>. He offers the course <a href="https://github.com/mosharaf/eecs598/tree/w21-ai">Systems for AI (EECS598)</a>.</li> </ul> <h2 id="system-group-new-york-university"><a href="http://www.news.cs.nyu.edu/">System Group (New York University)</a></h2> <ul> <li><a href="https://cims.nyu.edu/people/profiles/LI_Jinyang.html">Jinyang Li: </a> She is the Ph.D. advisor of <a href="https://jermainewang.github.io/">Dr. Minjie Wang</a>(the author of DGL).</li> </ul> <h2 id="shivaram-venkataraman-research-group-university-of-wisconsin-madison"><a href="https://shivaram.org/">Shivaram Venkataraman Research Group (University of Wisconsin, Madison)</a></h2> <ul> <li><a href="https://shivaram.org/">Shivaram Venkataraman: </a> Prof. Shivaram is the student of <a href="https://people.eecs.berkeley.edu/~istoica/">Prof. Ion Stoica</a>. He understands more about machine learning and less about systems. The papers he published is not too many, but the workload is substantial.</li> </ul> <h2 id="ecosystem-university-of-toronto"><a href="https://www.cs.toronto.edu/ecosystem/">EcoSystem (University of Toronto)</a></h2> <ul> <li><a href="https://www.cs.toronto.edu/~pekhimenko/">Gennady Pekhimenko: </a></li> </ul> <h1 id="-china">🇨🇳 China</h1> <p>In mainland China, it seems that most of the work in MLSys is being done in companies. However, some strong teams in distributed systems often also work on MLSys to some extent, such as <a href="https://ipads.se.sjtu.edu.cn/zh/index.html">IPADS (Shanghai Jiaotong Univeristy)</a>.</p> <h2 id="microsoft-research-labasia"><a href="https://www.microsoft.com/en-us/research/group/systems-and-networking-research-group-asia/">Microsoft Research Lab(Asia)</a></h2> <h2 id="pacman-grouptsinghua"><a href="https://pacman.cs.tsinghua.edu.cn/">PACMAN Group(Tsinghua)</a></h2> <p>More related to Arch</p> <h2 id="center-for-energy-efficient-computing-and-applicationspeking-university"><a href="https://ceca.pku.edu.cn/people/index.htm">Center for Energy-efficient Computing and Applications(Peking University)</a></h2> <p>More related to Arch</p> <h2 id="cheng-lis-research-group-univeristy-of-science-and-technology-china"><a href="http://staff.ustc.edu.cn/~chengli7">Cheng LI’s Research Group (Univeristy of Science and Technology China)</a></h2> <h2 id="ipadsshanghai-jiaotong-univeristy"><a href="https://ipads.se.sjtu.edu.cn/zh/index.html">IPADS(Shanghai Jiaotong Univeristy)</a></h2> <p>The best System Lab in Mainland China, and now is also working on some MLSys projects.</p> <h1 id="-appendix">🌟 Appendix</h1> <h2 id="some-people-worth-following-on-zhihu">Some people worth following on Zhihu</h2> <ul> <li> <p><a href="https://www.zhihu.com/people/crowowrk">Tianqi Chen: </a> Dr. Tianqi Chen is currently an Assistant Professor at Carnegie Mellon University. He helps run the <a href="https://catalyst.cs.cmu.edu/">Catalyst Group</a>.</p> </li> <li> <p><a href="https://www.zhihu.com/people/zhanghuaizheng">Huaizheng Zhang: </a> Dr. Huaizheng Zhang’s <a href="https://github.com/HuaizhengZhang/AI-System-School">AI-System-School</a> is an open project aimed at collecting and organizing research papers, tools, and resources related to MLSys, Large Language Models (LLM), and Generative AI (GenAI). It provides researchers and engineers with a systematic learning path and practical guide to help them better understand and apply these cutting-edge technologies. Here is his <a href="https://huaizheng.xyz/">personal website</a>.</p> </li> <li> <p><a href="https://www.zhihu.com/people/breaknever">Yue Zhao: </a> Dr. Yue Zhang is currently an Assistant Professor at the University of Southern California. He was also a student of Prof. Zhihao Jia. Here is his <a href="https://github.com/yzhao062">personal website</a>.</p> </li> </ul>]]></content><author><name></name></author><category term="MLSys-Learning-Journal"/><category term="MLSys-Learning-Journal"/><summary type="html"><![CDATA[A brief introduction to top-tier MLSys labs for Ph.D. applicants.]]></summary></entry></feed>